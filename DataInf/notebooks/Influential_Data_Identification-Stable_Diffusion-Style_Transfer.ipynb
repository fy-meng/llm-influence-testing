{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6920c3fa-66e3-442d-8afc-564a42aac27f",
   "metadata": {},
   "source": [
    "# Influential data identification - Stable_Diffusion - Style_Transfer\n",
    "\n",
    "This notebook demonstrates how to efficiently compute the influence functions using DataInf, showing its application to **influential data identification** tasks.\n",
    "\n",
    "- Model: [Stable Diffusion v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n",
    "- Fine-tuning dataset: [A style_transfer dataset](https://huggingface.co/datasets/kewu93/three_styles_prompted_250_512x512) that combined three different styles (cartoon, sketch, and pixel-art).\n",
    "\n",
    "References\n",
    "- `diffusers` HuggingFace library [[Link]](https://huggingface.co/docs/diffusers).\n",
    "- DataInf is available at this [ArXiv link](https://arxiv.org/abs/2310.00902)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3d2ef5-40ad-4910-a11c-b1a538f1533a",
   "metadata": {},
   "source": [
    "## Fine-tune a text-to-image model\n",
    "- We fine-tune a stable-diffusion-v1-5 model on a style-transfer dataset. We use `src/train_text_to_image_lora.py`, which is built on HuggingFace's [example](https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/train_text_to_image_lora.py). \n",
    "- The following code fine-tunes the model. If you want to skip this part, we can simply load fine-tuned weights at [this link](https://huggingface.co/kewu93/three_styles_lora)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "798ca648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !accelerate launch /PATH_TO_DataInf/DataInf/src/train_text_to_image_lora.py \\\n",
    "#   --pretrained_model_name_or_path=runwayml/stable-diffusion-v1-5 \\\n",
    "#   --dataset_name=kewu93/three_styles_prompted_250_512x512 \\ \n",
    "#   --resolution=512 --center_crop --random_flip \\\n",
    "#   --train_batch_size=1 \\\n",
    "#   --gradient_accumulation_steps=4 \\\n",
    "#   --max_train_steps=10000 \\\n",
    "#   --learning_rate=1e-04 \\\n",
    "#   --max_grad_norm=1 \\\n",
    "#   --lr_scheduler=\"cosine\" --lr_warmup_steps=0 \\\n",
    "#   --output_dir=/PATH_TO_OUTPUT_DIR/three_styles_lora \\\n",
    "#   --checkpointing_steps=1000 \\\n",
    "#   --validation_prompt=\"A sports car driving down a windy road.\" \\\n",
    "#   --seed=1337 \\\n",
    "#   --rank=2 \\\n",
    "#   --resume_from_checkpoint=\"latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4614009-4eac-4451-8601-620656a5b019",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from datasets import load_dataset\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from diffusers import AutoencoderKL, DDPMScheduler, StableDiffusionPipeline, DiffusionPipeline\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "from influence import IFEngineGeneration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c5cdd6-c26c-47cf-8df7-6d788984604a",
   "metadata": {},
   "source": [
    "## Load a fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c6671f4-6871-4450-a877-b8da44285ba7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "323464a0107144788f50db9e56c2e82c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n"
     ]
    }
   ],
   "source": [
    "model_base = \"runwayml/stable-diffusion-v1-5\"\n",
    "tokenizer = CLIPTokenizer.from_pretrained(model_base, subfolder=\"tokenizer\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(model_base, subfolder=\"text_encoder\")\n",
    "vae = AutoencoderKL.from_pretrained(model_base, subfolder=\"vae\").cuda()\n",
    "noise_scheduler = DDPMScheduler.from_pretrained(model_base, subfolder=\"scheduler\")\n",
    "\n",
    "'''\n",
    "Load Lora-tuned Unet\n",
    "'''\n",
    "pipeline = DiffusionPipeline.from_pretrained(model_base)\n",
    "pipeline.load_lora_weights(\"kewu93/three_styles_lora\") # publicly available weights!\n",
    "unet=pipeline.unet\n",
    "\n",
    "for param in unet.named_parameters():\n",
    "    param[1].requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66b0345-550f-4e31-a8ee-6e4ad181b542",
   "metadata": {},
   "source": [
    "## Load datasets and data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a86d12c-ceb1-4e5d-af9f-ce38fb2c0147",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Load Datasets\n",
    "'''\n",
    "\n",
    "def tokenize_captions(examples, is_train=True):\n",
    "    captions = []\n",
    "    for caption in examples['text']:\n",
    "        if isinstance(caption, str):\n",
    "            captions.append(caption)\n",
    "        elif isinstance(caption, (list, np.ndarray)):\n",
    "            # take a random caption if there are multiple\n",
    "            captions.append(random.choice(caption) if is_train else caption[0])\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Caption column `'text'` should contain either strings or lists of strings.\"\n",
    "            )\n",
    "    inputs = tokenizer(\n",
    "        captions, max_length=tokenizer.model_max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "    return inputs.input_ids\n",
    "\n",
    "train_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((512, 512), interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def preprocess_train(examples):\n",
    "    images = [image.convert(\"RGB\") for image in examples['image']]\n",
    "    examples[\"pixel_values\"] = [train_transforms(image) for image in images]\n",
    "    examples[\"input_ids\"] = tokenize_captions(examples)\n",
    "    return examples\n",
    "\n",
    "dataset_name = 'kewu93/three_styles_prompted_250_512x512'\n",
    "dataset = load_dataset(dataset_name)\n",
    "\n",
    "train_dataset = dataset[\"train\"].with_transform(preprocess_train)\n",
    "val_dataset = dataset[\"val\"].with_transform(preprocess_train)\n",
    "\n",
    "\n",
    "'''\n",
    "Create Data Loaders\n",
    "'''\n",
    "\n",
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()\n",
    "    input_ids = torch.stack([example[\"input_ids\"] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"input_ids\": input_ids}\n",
    "    \n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    batch_size=1,\n",
    "    num_workers=1,\n",
    ")\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    batch_size=1,\n",
    "    num_workers=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7a97e5-b0ce-4ac4-8666-f12b28793049",
   "metadata": {},
   "source": [
    "## Compute the gradient\n",
    " - Influence function uses the first-order gradient of a loss function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2528c51b-749f-4bc9-a573-216dca5e6508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "train\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "600it [48:52,  4.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "val\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "150it [12:08,  4.86s/it]\n"
     ]
    }
   ],
   "source": [
    "name_list = ['train', 'val']\n",
    "gradient_dict={}\n",
    "for idx, dataloader_ in enumerate([train_dataloader, val_dataloader]):\n",
    "    print('-'*30)\n",
    "    print(name_list[idx])\n",
    "    print('-'*30)\n",
    "    unet.train()\n",
    "    unet = unet.cuda()\n",
    "    grad_dict = {}\n",
    "    for step, batch in tqdm(enumerate(dataloader_)):\n",
    "        torch.manual_seed(step)\n",
    "        grad_dict_one_sample={}\n",
    "        for layer_name, layer_weights in unet.named_parameters():\n",
    "            if 'lora_' in layer_name:\n",
    "                grad_dict_one_sample[layer_name] = []\n",
    "\n",
    "        for timestep_ in [25, 225, 425, 525, 725, 925]:\n",
    "            unet.zero_grad()\n",
    "            latents = vae.encode(batch[\"pixel_values\"].cuda()).latent_dist.sample().cuda()\n",
    "            latents = latents * vae.config.scaling_factor\n",
    "            noise = torch.randn_like(latents).cuda()\n",
    "            bsz = latents.shape[0]\n",
    "            timesteps = torch.LongTensor([timestep_]).cuda()\n",
    "            noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps).cuda()\n",
    "            encoder_hidden_states = text_encoder(batch[\"input_ids\"])[0].cuda()\n",
    "            target = noise\n",
    "            model_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
    "            loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\n",
    "            loss.backward()\n",
    "            for layer_name, layer_weights in unet.named_parameters():\n",
    "                if 'lora_A' in layer_name:\n",
    "                    grad_dict_one_sample[layer_name].append(layer_weights.grad.cpu())\n",
    "                elif 'lora_B' in layer_name:\n",
    "                    # first index of shape indicates low-rank\n",
    "                    grad_dict_one_sample[layer_name].append(layer_weights.grad.T.cpu())\n",
    "                else:\n",
    "                    pass\n",
    "                if 'lora_' in layer_name and timestep_ == 925:\n",
    "                    grad_dict_one_sample[layer_name] = torch.cat(grad_dict_one_sample[layer_name])            \n",
    "\n",
    "        grad_dict[step]=grad_dict_one_sample\n",
    "        del latents, noise, bsz, timesteps, noisy_latents, encoder_hidden_states, target, model_pred, loss\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    gradient_dict[name_list[idx]]=grad_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4912db0b-353a-43a6-a34e-47be6f0d9dad",
   "metadata": {},
   "source": [
    "## Compute the influence function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36615aa0-6ff5-4db0-9999-6123e02443d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 150/150 [23:48<00:00,  9.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing IF for method:  identity\n",
      "Computing IF for method:  proposed\n"
     ]
    }
   ],
   "source": [
    "influence_engine = IFEngineGeneration()\n",
    "influence_engine.preprocess_gradients(gradient_dict['train'], gradient_dict['val'])\n",
    "influence_engine.compute_hvps()\n",
    "influence_engine.compute_IF()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fa6f69-7679-4a7a-8662-f9763e60a027",
   "metadata": {},
   "source": [
    "## Application to influential data detection task\n",
    "### AUC and Recall "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3337271-08a9-48e9-80ad-3beab0561bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "identity_df=influence_engine.IF_dict['identity']\n",
    "proposed_df=influence_engine.IF_dict['proposed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2fe232b-e7e2-4e1e-b0fa-7bb029ae84d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "identity AUC: 0.612/0.079\n",
      "proposed AUC: 0.599/0.077\n"
     ]
    }
   ],
   "source": [
    "identity_auc_list, proposed_auc_list=[], []\n",
    "for i in range(len(dataset[\"val\"]['style_class'])):\n",
    "    gt_label=dataset[\"val\"]['style_class'][i]\n",
    "    gt_array=np.array([1 if tr_label == gt_label else 0 for tr_label in dataset[\"train\"]['style_class']])\n",
    "    \n",
    "    # The influence function is anticipated to have a big negative value when its class equals to a validation data point. \n",
    "    # This is because a data point with the same class is likely to be more helpful in minimizing the validation loss.\n",
    "    # Thus, we multiply the influence function value by -1 to account for alignment with the gt_array. \n",
    "    identity_auc_list.append(roc_auc_score(gt_array, -(identity_df.iloc[i,:].to_numpy())))\n",
    "    proposed_auc_list.append(roc_auc_score(gt_array, -(proposed_df.iloc[i,:].to_numpy())))\n",
    "    \n",
    "print(f'identity AUC: {np.mean(identity_auc_list):.3f}/{np.std(identity_auc_list):.3f}')\n",
    "print(f'proposed AUC: {np.mean(proposed_auc_list):.3f}/{np.std(proposed_auc_list):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "292ceff3-be14-4544-8283-1edbe8af8d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "identity Recall: 0.889/0.173\n",
      "proposed Recall: 0.916/0.109\n"
     ]
    }
   ],
   "source": [
    "# Recall calculations\n",
    "val_array=np.array(dataset['val']['style_class'])\n",
    "identity_recall_list, proposed_recall_list=[], []\n",
    "for i in range(len(dataset[\"val\"]['style_class'])):\n",
    "    gt_label=dataset[\"val\"]['style_class'][i]\n",
    "    n_label=np.sum(val_array == gt_label)\n",
    "    \n",
    "    sorted_index=np.argsort(identity_df.iloc[i].values) # ascending order\n",
    "    sorted_array=np.array([dataset[\"train\"]['style_class'][j] for j in sorted_index])\n",
    "    recall_identity=np.count_nonzero(sorted_array[:n_label] == gt_label)/n_label\n",
    "    identity_recall_list.append(recall_identity)\n",
    "    \n",
    "    sorted_index=np.argsort(proposed_df.iloc[i].values) # ascending order\n",
    "    sorted_array=np.array([dataset[\"train\"]['style_class'][j] for j in sorted_index])\n",
    "    recall_proposed=np.count_nonzero(sorted_array[:n_label] == gt_label)/n_label\n",
    "    proposed_recall_list.append(recall_proposed)\n",
    "    \n",
    "print(f'identity Recall: {np.mean(identity_recall_list):.3f}/{np.std(identity_recall_list):.3f}')\n",
    "print(f'proposed Recall: {np.mean(proposed_recall_list):.3f}/{np.std(proposed_recall_list):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde7603f-d9a1-4480-93d8-dc42daf745b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069fe912-617f-440e-9e43-e216f701f1cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
